{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1t8ghmNAUpat_cPI1nQLv62INp2nV3Rnv","authorship_tag":"ABX9TyPW7KRVfNwWYH2dTvyewunQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Gp4eyjxOUBXK","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1723095369250,"user_tz":-330,"elapsed":547,"user":{"displayName":"Abhishek maher","userId":"17060706371112675718"}},"outputId":"fbf5bdce-3b9e-46eb-f039-37539ac67f8d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n1. Tokenize the words\\n3. each category should have 7500 words\\n2. use glove to make embeddings\\n4. encode emoji labels\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}],"source":["\"\"\"\n","To-Do list:\n","\n","1. Tokenize the words\n","2. each category should have 7500 words\n","3. use glove to make embeddings\n","4. encode emoji labels\n","\"\"\""]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from nltk.tokenize import word_tokenize\n","import re\n","\n","# import these modules\n","from nltk.stem import WordNetLemmatizer\n","import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"63z3tX0154A-","executionInfo":{"status":"ok","timestamp":1723095378026,"user_tz":-330,"elapsed":7466,"user":{"displayName":"Abhishek maher","userId":"17060706371112675718"}},"outputId":"a3480a7d-0299-4da2-e798-4ca76fb504b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/emojien/processed_tweets.csv')"],"metadata":{"id":"TlJcKxf26VzB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = df.dropna()"],"metadata":{"id":"lb2jo85kV1xt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lemmatizer = WordNetLemmatizer()"],"metadata":{"id":"xoqQqNbhQFQR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def normalize_stretched_word(word):\n","    return re.sub(r'(.)\\1+', r'\\1\\1', word)  # Replace repeated characters with two occurrences\n","\n","def lemmatize_and_normalize_text(text):\n","    tokens = word_tokenize(text.lower())  # Tokenize and convert to lowercase\n","    normalized_tokens = [normalize_stretched_word(token) for token in tokens]\n","    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in normalized_tokens]\n","    return ' '.join(lemmatized_tokens)"],"metadata":{"id":"no8qQ-mD6Z8M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['processed_text'] = df['processed_text'].apply(lemmatize_and_normalize_text)"],"metadata":{"id":"aE2bDMLbQziP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(df['processed_text'])"],"metadata":{"id":"EUneHlIH6b5m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["word_index = tokenizer.word_index\n","vocab_size = len(word_index) + 1"],"metadata":{"id":"WWwYOUkx_8w4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_size #147913"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0a0nbmEjR4vY","executionInfo":{"status":"ok","timestamp":1723095726893,"user_tz":-330,"elapsed":33,"user":{"displayName":"Abhishek maher","userId":"17060706371112675718"}},"outputId":"39bc38b0-dd43-49ab-e499-11caf274a624"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["142180"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# Loading pre-trained glove word embeddings\n","\n","def load_glove_embeddings(file_path):\n","    embeddings_index = {}\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            values = line.split()\n","            word = values[0]\n","            try:\n","                coefs = np.asarray(values[1:], dtype='float32')\n","                embeddings_index[word] = coefs\n","            except ValueError as e:\n","                print(f\"Skipping line due to error: {e}\")\n","                continue\n","    return embeddings_index\n","\n","glove_file_path = '/content/drive/MyDrive/glove/glove_embeddings/glove.840B.300d.txt'  # Replace with your actual path to your GloVe file\n","embeddings_index = load_glove_embeddings(glove_file_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cNspvCsnA6hI","executionInfo":{"status":"ok","timestamp":1723095940800,"user_tz":-330,"elapsed":211033,"user":{"displayName":"Abhishek maher","userId":"17060706371112675718"}},"outputId":"19ad5208-7cf1-44d5-d75e-0b875b1da309"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Skipping line due to error: could not convert string to float: '.'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: '.'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: '.'\n","Skipping line due to error: could not convert string to float: '.'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: 'Killerseats.com'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: 'mylot.com'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: 'Amazon.com'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n"]}]},{"cell_type":"code","source":["embedding_dim = 300  # For GloVe 100d\n","embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n","\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        # Words not found in embedding index will be all-zeros\n","        embedding_matrix[i] = embedding_vector"],"metadata":{"id":"rjwv-Od9EaCi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['sequences'] = tokenizer.texts_to_sequences(df['processed_text'])"],"metadata":{"id":"2YIzqlwIGqWx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["empty_rows = np.all(embedding_matrix == 0, axis=1) #no. of tokens with no word embedding\n","np.sum(empty_rows)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I1FaWSoULAN8","executionInfo":{"status":"ok","timestamp":1723096062320,"user_tz":-330,"elapsed":35,"user":{"displayName":"Abhishek maher","userId":"17060706371112675718"}},"outputId":"eeff3b70-1f1a-41f1-cad4-c9f3fb105d77"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["71647"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["df['emoji_len'] = df['emojis'].apply(len)\n","\n","df = df[df['emoji_len'] < 10] #removing rows with more than 10 emojis"],"metadata":{"id":"o8wD9KB8N9X5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["emojis = df.explode('emojis')['emojis'].unique()"],"metadata":{"id":"pGMpF-b9gx14"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_df = pd.DataFrame()"],"metadata":{"id":"OmH4vJR8hMiq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.utils import resample\n","\n","# Resample each emoji category to ensure exactly 7500 texts, then shuffle and concatenate them.\n","for emoji in emojis:\n","    emoji_data = df[df['emojis'] == emoji]\n","    emoji_count = len(emoji_data)\n","\n","    if emoji_count > 7500:\n","        emoji_df = emoji_data.sample(n=7500, random_state=42)\n","\n","    elif emoji_count < 7500:\n","        emoji_df = resample(emoji_data, replace=True, n_samples=7500, random_state=42)\n","\n","    else:\n","        emoji_df = emoji_data\n","\n","    emoji_df = emoji_df.sample(frac=1, random_state=42)\n","\n","    new_df = pd.concat([new_df, emoji_df], ignore_index=True)"],"metadata":{"id":"SPWyqsqvh_Kj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import OneHotEncoder\n","\n","# One-hot encode the emojis column\n","\n","one_hot_encoder = OneHotEncoder(sparse_output=False)\n","one_hot_encoded = one_hot_encoder.fit_transform(new_df[['emojis']])"],"metadata":{"id":"kWDHLSsHkHzu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["categories = one_hot_encoder.categories_[0]\n","index_to_emoji = {index: emoji for index, emoji in enumerate(categories)} # Create a dictionary to map indices to emojis"],"metadata":{"id":"WKhC3sdblCQt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_df['emoji_labels'] = one_hot_encoded.tolist()"],"metadata":{"id":"5O43wqv7jcEa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = new_df[['sequences', 'emoji_labels']]"],"metadata":{"id":"yyPbZbbXjfJn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['sequences'].apply(len).max()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4BGjVDyAkK5s","executionInfo":{"status":"ok","timestamp":1723096687351,"user_tz":-330,"elapsed":535,"user":{"displayName":"Abhishek maher","userId":"17060706371112675718"}},"outputId":"ee0d6faf-13e8-48ee-d177-a88addcfef3e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["47"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","max_sequence_length = 47\n","data['padded_sequences'] = pad_sequences(data['sequences'], maxlen=max_sequence_length, padding='pre').tolist() # Pad sequences to a fixed length - 47"],"metadata":{"id":"iL_imuNijnpE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","with open('/content/drive/MyDrive/emojien/Data/emoji_data.pkl', 'wb') as f:\n","    pickle.dump({\n","        'data': data,\n","        'tokenizer': tokenizer,\n","        'emoji_dict': index_to_emoji,\n","        'embedding_matrix': embedding_matrix\n","    }, f)"],"metadata":{"id":"aY3TWpBPla1n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FSMeLD-7mszy"},"execution_count":null,"outputs":[]}]}