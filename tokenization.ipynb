{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":547,"status":"ok","timestamp":1723095369250,"user":{"displayName":"Abhishek maher","userId":"17060706371112675718"},"user_tz":-330},"id":"Gp4eyjxOUBXK","outputId":"fbf5bdce-3b9e-46eb-f039-37539ac67f8d"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n1. Tokenize the words\\n3. each category should have 7500 words\\n2. use glove to make embeddings\\n4. encode emoji labels\\n'"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","To-Do list:\n","\n","1. Tokenize the words\n","2. each category should have 7500 words\n","3. use glove to make embeddings\n","4. encode emoji labels\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7466,"status":"ok","timestamp":1723095378026,"user":{"displayName":"Abhishek maher","userId":"17060706371112675718"},"user_tz":-330},"id":"63z3tX0154A-","outputId":"a3480a7d-0299-4da2-e798-4ca76fb504b9"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import numpy as np\n","\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from nltk.tokenize import word_tokenize\n","import re\n","\n","# import these modules\n","from nltk.stem import WordNetLemmatizer\n","import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TlJcKxf26VzB"},"outputs":[],"source":["df = pd.read_csv('./Data/processed_tweets.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lb2jo85kV1xt"},"outputs":[],"source":["df = df.dropna()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xoqQqNbhQFQR"},"outputs":[],"source":["lemmatizer = WordNetLemmatizer()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"no8qQ-mD6Z8M"},"outputs":[],"source":["def normalize_stretched_word(word):\n","    return re.sub(r'(.)\\1+', r'\\1\\1', word)  # Replace repeated characters with two occurrences\n","\n","def lemmatize_and_normalize_text(text):\n","    tokens = word_tokenize(text.lower())  # Tokenize and convert to lowercase\n","    normalized_tokens = [normalize_stretched_word(token) for token in tokens]\n","    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in normalized_tokens]\n","    return ' '.join(lemmatized_tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aE2bDMLbQziP"},"outputs":[],"source":["df['processed_text'] = df['processed_text'].apply(lemmatize_and_normalize_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EUneHlIH6b5m"},"outputs":[],"source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(df['processed_text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WWwYOUkx_8w4"},"outputs":[],"source":["word_index = tokenizer.word_index\n","vocab_size = len(word_index) + 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1723095726893,"user":{"displayName":"Abhishek maher","userId":"17060706371112675718"},"user_tz":-330},"id":"0a0nbmEjR4vY","outputId":"39bc38b0-dd43-49ab-e499-11caf274a624"},"outputs":[{"data":{"text/plain":["142180"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["vocab_size #147913"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":211033,"status":"ok","timestamp":1723095940800,"user":{"displayName":"Abhishek maher","userId":"17060706371112675718"},"user_tz":-330},"id":"cNspvCsnA6hI","outputId":"19ad5208-7cf1-44d5-d75e-0b875b1da309"},"outputs":[{"name":"stdout","output_type":"stream","text":["Skipping line due to error: could not convert string to float: '.'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: '.'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: '.'\n","Skipping line due to error: could not convert string to float: '.'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: 'Killerseats.com'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: 'mylot.com'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: 'Amazon.com'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n"]}],"source":["# Loading pre-trained glove word embeddings\n","\n","def load_glove_embeddings(file_path):\n","    embeddings_index = {}\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            values = line.split()\n","            word = values[0]\n","            try:\n","                coefs = np.asarray(values[1:], dtype='float32')\n","                embeddings_index[word] = coefs\n","            except ValueError as e:\n","                print(f\"Skipping line due to error: {e}\")\n","                continue\n","    return embeddings_index\n","\n","glove_file_path = 'path/to/glove.840B.300d.txt'  # Download glove embedding file and replace with your actual path\n","embeddings_index = load_glove_embeddings(glove_file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rjwv-Od9EaCi"},"outputs":[],"source":["embedding_dim = 300  # For GloVe 100d\n","embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n","\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        # Words not found in embedding index will be all-zeros\n","        embedding_matrix[i] = embedding_vector"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2YIzqlwIGqWx"},"outputs":[],"source":["df['sequences'] = tokenizer.texts_to_sequences(df['processed_text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35,"status":"ok","timestamp":1723096062320,"user":{"displayName":"Abhishek maher","userId":"17060706371112675718"},"user_tz":-330},"id":"I1FaWSoULAN8","outputId":"eeff3b70-1f1a-41f1-cad4-c9f3fb105d77"},"outputs":[{"data":{"text/plain":["71647"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["empty_rows = np.all(embedding_matrix == 0, axis=1) #no. of tokens with no word embedding\n","np.sum(empty_rows)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o8wD9KB8N9X5"},"outputs":[],"source":["df['emoji_len'] = df['emojis'].apply(len)\n","\n","df = df[df['emoji_len'] < 10] #removing rows with more than 10 emojis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pGMpF-b9gx14"},"outputs":[],"source":["emojis = df.explode('emojis')['emojis'].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OmH4vJR8hMiq"},"outputs":[],"source":["new_df = pd.DataFrame()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SPWyqsqvh_Kj"},"outputs":[],"source":["from sklearn.utils import resample\n","\n","# Resample each emoji category to ensure exactly 7500 texts, then shuffle and concatenate them.\n","for emoji in emojis:\n","    emoji_data = df[df['emojis'] == emoji]\n","    emoji_count = len(emoji_data)\n","\n","    if emoji_count > 7500:\n","        emoji_df = emoji_data.sample(n=7500, random_state=42)\n","\n","    elif emoji_count < 7500:\n","        emoji_df = resample(emoji_data, replace=True, n_samples=7500, random_state=42)\n","\n","    else:\n","        emoji_df = emoji_data\n","\n","    emoji_df = emoji_df.sample(frac=1, random_state=42)\n","\n","    new_df = pd.concat([new_df, emoji_df], ignore_index=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kWDHLSsHkHzu"},"outputs":[],"source":["from sklearn.preprocessing import OneHotEncoder\n","\n","# One-hot encode the emojis column\n","\n","one_hot_encoder = OneHotEncoder(sparse_output=False)\n","one_hot_encoded = one_hot_encoder.fit_transform(new_df[['emojis']])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WKhC3sdblCQt"},"outputs":[],"source":["categories = one_hot_encoder.categories_[0]\n","index_to_emoji = {index: emoji for index, emoji in enumerate(categories)} # Create a dictionary to map indices to emojis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5O43wqv7jcEa"},"outputs":[],"source":["new_df['emoji_labels'] = one_hot_encoded.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yyPbZbbXjfJn"},"outputs":[],"source":["data = new_df[['sequences', 'emoji_labels']]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":535,"status":"ok","timestamp":1723096687351,"user":{"displayName":"Abhishek maher","userId":"17060706371112675718"},"user_tz":-330},"id":"4BGjVDyAkK5s","outputId":"ee0d6faf-13e8-48ee-d177-a88addcfef3e"},"outputs":[{"data":{"text/plain":["47"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["df['sequences'].apply(len).max()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iL_imuNijnpE"},"outputs":[],"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","max_sequence_length = 47\n","data['padded_sequences'] = pad_sequences(data['sequences'], maxlen=max_sequence_length, padding='pre').tolist() # Pad sequences to a fixed length - 47"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aY3TWpBPla1n"},"outputs":[],"source":["import pickle\n","\n","with open('./Data/emoji_data.pkl', 'wb') as f:\n","    pickle.dump({\n","        'data': data,\n","        'tokenizer': tokenizer,\n","        'emoji_dict': index_to_emoji,\n","        'embedding_matrix': embedding_matrix\n","    }, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FSMeLD-7mszy"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPW7KRVfNwWYH2dTvyewunQ","mount_file_id":"1t8ghmNAUpat_cPI1nQLv62INp2nV3Rnv","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
